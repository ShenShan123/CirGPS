import torch
from torch import Tensor
from tqdm import tqdm
# from torch_geometric.utils import k_hop_subgraph #, drnl_node_labeling
from torch_geometric.data import Data
# from torch_geometric.utils import from_dgl
from torch_geometric.utils import to_undirected, structured_negative_sampling, negative_sampling
import logging
from torch_geometric.data.collate import collate
import os
import psutil
import time
import copy
import torch.multiprocessing as mp
from torch_geometric.data.separate import separate
import datetime

""" This version is based on the dataset from dgl/exmaple/pytorch/obg/ngnn_seal """

def get_pos_neg_edges(
        g, sample_type='structured', 
        force_undirected=False, 
        neg_ratio=1.0,
    ):
    r""" we got 3 types target edges so far, cc_p2n, cc_p2p, cc_n2n.
    So we need to generate negative edges for each target edge type.
    This is a balanced data version.
    Args:
        g (pyg graph): the orignal homogenous graph.
        type (string): 'global' sampling or 'structured' sampling.
        force_undirected (bool): whether negative edges are undirected
        neg_ratio (float): average num of negative edges per positive edge
        sample_ratio (float): sample ratio for all edges
    Return:
        pos_edge_index (LongTensor 2xN), neg_edge_index (LongTensor 2xN),
        neg_edge_type (LongTensor N)
    """
    if neg_ratio > 1.0 or sample_type == 'global':
        neg_edge_index = negative_sampling(
            g.tar_edge_index, g.num_nodes,
            force_undirected=force_undirected,
            num_neg_samples=int(g.tar_edge_index.size(1)*neg_ratio),
        )
        
        neg_edge_type = torch.zeros(neg_edge_index.size(1), dtype=torch.long)
        for i in range(neg_edge_index.size(1)):
            node_pair = neg_edge_index[:, i]
            ntypes = set(g.node_type[node_pair].tolist())
            # for neg edge types are related to target edge types
            # weight for neg Cc_p2n
            if ntypes == {0, 2}: 
                neg_edge_type[i] = 2
            # weight for Cc_p2p
            elif ntypes == {2}:
                neg_edge_type[i] = 3
            # weight for Cc_n2n
            elif ntypes == {0}:
                neg_edge_type[i] = 4
        
        legal_mask = neg_edge_type > 0
        logging.info(
            f"Using global negtive sampling, #pos={g.tar_edge_index.size(1)}, " + 
            f"#neg={neg_edge_index[:, legal_mask].size(1)}")
        return g.tar_edge_index, neg_edge_index[:,legal_mask], neg_edge_type[legal_mask]
    
    neg_edge_index = []
    neg_edge_type  = []
    tar_edge_offset = 0
    for i in range(g.tar_edge_dist.size(0)):
        pos_edges = g.tar_edge_index[:, tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset]
        pos_edge_src, pos_edge_dst, neg_edge_dst = structured_negative_sampling(
            pos_edges, g.num_nodes, contains_neg_self_loops=False,
        )
        tar_edge_offset += g.tar_edge_dist[i]
        # neg edge sampling
        indices = torch.randperm(neg_edge_dst.size(0))[
            :int(neg_edge_dst.size(0) * neg_ratio), 
        ]
        neg_edge_index.append(
            torch.stack((pos_edge_src[indices], neg_edge_dst[indices]), dim=0)
        )
        neg_edge_type += [i + g.tar_edge_type[0]] * indices.size(0)

        logging.info(
            f"Using structured negtive sampling for target etype {i}, " + 
            f"pos={pos_edges.size(1)}, #neg={neg_edge_index[-1].size(1)}")
    
    return torch.cat(neg_edge_index, 1), torch.tensor(neg_edge_type)

def get_balanced_edges(
    g, neg_edge_index, neg_edge_type,
    neg_edge_ratio, sample_ratio = 1.0,
):
    r""" Get balanced edges according to their etypes, including both pos & neg edges.
    
    Args:
        g (Data): provides positive target edges and edge types.
        neg_edge_index (Tensor[2 x N]): neg edges generated by get_pos_neg_edges().
        neg_edge_type (Tensor[N]): neg edge types corresponding to pos edges.
        neg_edge_ratio (float): #neg edges per pos edge.
        sample_ratio (float): the proportion of samples to train.
    
    Return:
        pos_edge_index (Tensor [2, Npos]).
        pos_edge_type (Tensor [Npos]).
        pos_edge_y (Tensor [Npos]): target Cc values.
        neg_edge_index (Tensor [2, Nneg]).
        neg_edge_type (Tensor [Nneg]).
    """
    tar_edge_offset = 0
    min_edge_num = g.tar_edge_dist.min()
    neg_edge_index_list = []
    neg_edge_type_list  = []
    pos_edge_index_list = []
    pos_edge_type_list  = []
    pos_edge_y_list  = []
    for i in range(g.tar_edge_dist.size(0)):
        # for pos edges
        pos_edges = g.tar_edge_index[
            :, tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset
        ]
        pos_etypes = g.tar_edge_type[
            tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset
        ]
        pos_edge_y = g.tar_edge_y[
            tar_edge_offset:g.tar_edge_dist[i]+tar_edge_offset
        ]
        tar_edge_offset += g.tar_edge_dist[i]

        indices = torch.randperm(pos_edges.size(1))[
            :int(min_edge_num * sample_ratio), 
        ]
        pos_edge_index_list.append(pos_edges[:, indices])
        pos_edge_type_list.append(pos_etypes[indices])
        pos_edge_y_list.append(pos_edge_y[indices])
        
        logging.info(f"Edge type {i}, balanced pos edge num: {pos_edge_type_list[-1].size(0)}")

        # for neg edges
        neg_edge_mask = (neg_edge_type - neg_edge_type.min()) == i
        assert neg_edge_index.size(1) == neg_edge_type.size(0)
        neg_edges = neg_edge_index[:, neg_edge_mask]
        neg_etypes = neg_edge_type[neg_edge_mask]
        indices = torch.randperm(neg_edges.size(1))[
            :int(min_edge_num * neg_edge_ratio * sample_ratio), 
        ]
        neg_edge_index_list.append(neg_edges[:, indices])
        neg_edge_type_list.append(neg_etypes[indices])

        logging.info(f"Edge type {i}, balanced neg edge num: {neg_edge_type_list[-1].size(0)}")

    return (torch.cat(pos_edge_index_list, 1), torch.cat(pos_edge_type_list), 
            torch.cat(pos_edge_y_list),  # these are target Cc values
            torch.cat(neg_edge_index_list, 1), torch.cat(neg_edge_type_list),)

def add_tar_edges_to_g(g, neg_edge_index, neg_edge_type):
        added_edges_index = torch.cat((g.tar_edge_index, neg_edge_index), 1)
        added_edge_type = torch.cat((g.tar_edge_type, neg_edge_type))
        if g.is_undirected():
            added_edges_index, added_edge_type = to_undirected(
                added_edges_index, added_edge_type, g.num_nodes, reduce='mean'
            )
        logging.info(f"#added edges={g.tar_edge_index.size(1)+neg_edge_index.size(1)} "+
                     f"#undirected added edges={added_edges_index.size(1)}")
        aug_g = Data()
        aug_g.edge_index = torch.cat((g.edge_index, added_edges_index), dim=1)
        aug_g.edge_type = torch.cat((g.edge_type, added_edge_type)).long()
        # aug_g.neg_edge_mask = aug_g.edge_type >= g._num_etypes
        aug_g.node_type = g.node_type.long()
        aug_g.node_attr = g.node_attr
        aug_g.num_pos_etype = g._num_etypes
        aug_g.num_ntypes = g._num_ntypes
        aug_g.tar_edge_type_offset = g.tar_edge_type.min()
        # aug_g.tar_edge_dist = g.tar_edge_dist
        # del g.tar_edge_index
        # del g.tar_edge_type
        print("DEBUG: aug_g", aug_g)
        return aug_g

def k_hop_subgraph_wo_target_edges(
    node_idx: Tensor,
    num_hops: int,
    edge_index: Tensor,
    relabel_nodes: bool = False,
    num_nodes: int = None,
    flow: str = 'source_to_target',
    directed: bool = False,
    sample_prob: float = 1.0,
) -> tuple:
    r"""Computes the induced subgraph of :obj:`edge_index` around all nodes in
    :attr:`node_idx` reachable within :math:`k` hops.

    The :attr:`flow` argument denotes the direction of edges for finding
    :math:`k`-hop neighbors. If set to :obj:`"source_to_target"`, then the
    method will find all neighbors that point to the initial set of seed nodes
    in :attr:`node_idx.`
    This mimics the natural flow of message passing in Graph Neural Networks.

    The method returns (1) the nodes involved in the subgraph, (2) the filtered
    :obj:`edge_index` connectivity, (3) the mapping from node indices in
    :obj:`node_idx` to their new location, and (4) the edge mask indicating
    which edges were preserved.
    Args:
        node_idx (int, list, tuple or :obj:`torch.Tensor`): The central seed
            node(s).
        num_hops (int): The number of hops :math:`k`.
        edge_index (LongTensor): The edge indices.
        relabel_nodes (bool, optional): If set to :obj:`True`, the resulting
            :obj:`edge_index` will be relabeled to hold consecutive indices
            starting from zero. (default: :obj:`False`)
        num_nodes (int, optional): The number of nodes, *i.e.*
            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)
        flow (str, optional): The flow direction of :math:`k`-hop aggregation
            (:obj:`"source_to_target"` or :obj:`"target_to_source"`).
            (default: :obj:`"source_to_target"`)
        directed (bool, optional): If set to :obj:`False`, will include all
            edges between all sampled nodes. (default: :obj:`True`)

    :rtype: (:class:`LongTensor`, :class:`LongTensor`, :class:`LongTensor`,
             :class:`BoolTensor`)
    """

    # num_nodes = maybe_num_nodes(edge_index, num_nodes)

    assert flow in ['source_to_target', 'target_to_source']
    if flow == 'target_to_source':
        row, col = edge_index
    else:
        col, row = edge_index

    node_mask = row.new_empty(num_nodes, dtype=torch.bool)
    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)

    # if isinstance(node_idx, (int, list, tuple)):
    #     node_idx = torch.tensor([node_idx], device=row.device).flatten()
    # else:
    node_idx = node_idx.to(row.device).view(-1)
    subsets = [node_idx]

    for _ in range(num_hops):
        node_mask.fill_(False)
        node_mask[subsets[-1]] = True
        torch.index_select(node_mask, 0, row, out=edge_mask)
        new_subset = col[edge_mask].unique()
        indices = torch.randperm(new_subset.size(0))[
            :int(new_subset.size(0)*sample_prob)]
        subsets.append(new_subset[indices])
    # print("DEBUG",subsets)
    subset, inv = torch.cat(subsets).unique(return_inverse=True)
    inv = inv[:node_idx.numel()]

    node_mask.fill_(False)
    node_mask[subset] = True

    if not directed:
        edge_mask = node_mask[row] & node_mask[col]

    # if we do sampling for link, we got node_idx.size(0)==2
    # otherwise, we do sampling for single node.
    if node_idx.size(0) > 1:
        # remove the target edges from the subgraph, added by shan
        src_dst_edge_mask  = (row == node_idx[0]) & (col == node_idx[1]) 
        src_dst_edge_mask |= (row == node_idx[1]) & (col == node_idx[0]) 
        edge_mask &= ~src_dst_edge_mask

    edge_index = edge_index[:, edge_mask]

    if relabel_nodes:
        node_idx = row.new_full((num_nodes, ), -1)
        node_idx[subset] = torch.arange(subset.size(0), device=row.device)
        edge_index = node_idx[edge_index]

    return subset, edge_index, inv, edge_mask
    
def seal_sampling(
    edge_index: Tensor, 
    edge_type: Tensor,
    node_type: Tensor,
    directed: bool,
    node_pairs: Tensor, labels: Tensor, targets: Tensor,
    num_hops: int, ratio_per_hop: float=1.0,
    node_attr: Tensor=None,
    stash_path: str=None,
    # weight: float=1.0,
) -> Data:
    # logger = logging.getLogger()
    # logger.setLevel(logging.INFO)
    # logger.addHandler(logging.StreamHandler())
    start = time.perf_counter()
    f = open(f"pid{os.getpid()}_" + 
             f"{datetime.datetime.now()}.log", 'w')
    # g, node_pair, label = arg_tuple
    # weight = 1.0
    num_nodes = node_type.size(0)
    data_list = []
    # this is links to be sample, otherwise is nodes
    if node_pairs.ndim == 2 and node_pairs.size(0) == 2:
        num_pairs = node_pairs.size(1)
    else:
        num_pairs = node_pairs.size(0)
    # found bug if we use tqdm
    # for i in tqdm(range(num_pairs), ncols=80):
    for i in range(num_pairs): # for links sampling
        if node_pairs.ndim == 2 and node_pairs.size(0) == 2:
            subset, subg_edge_index, mapped_node_pair, subg_edge_mask = \
            k_hop_subgraph_wo_target_edges(
                node_pairs[:, i], num_hops, edge_index, 
                relabel_nodes=True, num_nodes=num_nodes, 
                directed=directed,
                sample_prob=ratio_per_hop,
            )
            assert mapped_node_pair.size(0) == 2
        else: # for nodes sampling
            subset, subg_edge_index, mapped_node_pair, subg_edge_mask = \
            k_hop_subgraph_wo_target_edges(
                node_pairs[i], num_hops, edge_index, 
                relabel_nodes=True, num_nodes=num_nodes, 
                directed=directed,
                sample_prob=ratio_per_hop,
            )
            assert mapped_node_pair.size(0) == 1
            # print("DEBUG: sampling for 1 node", subset[mapped_node_pair], targets[i])

        if (mapped_node_pair.size(0) == 2 and subset.size(0) > 10000) or \
            (mapped_node_pair.size(0) == 2 and subset.size(0) > 4000):
            logstr = f"large subg {i}, centers {subset[mapped_node_pair]} " + \
                     f"#nodes {subset.size(0)}, #edges {subg_edge_index.size(1)}"
            f.write(logstr+"\n")
            f.flush()
            print(logstr)
            continue
        # print("return from seal_sampling")
        data = Data()
        data.edge_index = subg_edge_index
        data.mapped_node_pair = mapped_node_pair
        # data.edge_mask = subg_edge_mask.nonzero().view(-1)
        data.edge_attr = edge_type[subg_edge_mask].view(-1)
        data.y = labels[i].to(torch.int8)
        data.y_reg = targets[i]
        data.x = node_type[subset].view(-1, 1)
        # data.weight = weight
    
        if node_attr is not None:
            # same size(0) as data.x
            assert node_type.size(0) == node_attr.size(0)
            data.node_attr = node_attr[subset]
            data.node_type = node_type[subset].view(-1).to(torch.int8)
            # data.ori_node_index = subset

        data_list.append(data)

        vm = psutil.virtual_memory()            
        if i % 200 == 0:
            elapsed = time.perf_counter() - start
            timestr = time.strftime(
                '%H:%M:%S', time.gmtime(elapsed)) + f'{elapsed:.2f}'[-3:]
            
            logstr = \
                f">>> pid {os.getpid()} | " + \
                f"mem used {vm[3]/1e9:.2f}GB {vm[2]:.2f}% | " + \
                f"progress {i}/{num_pairs}  {100*float(i/num_pairs):.2f}% | " + \
                f"took {timestr} | {datetime.datetime.now()}"
            f.write(logstr+"\n")
            f.flush()
            print(logstr)
            # time.sleep(10)
            
        
        if vm[2] > 85.0:
            logstr = \
                "Break sampling loop due to short of memory: " + \
                f"{vm[3]/1e9:.2f}GB {vm[2]:.2f}%, data_list length {len(data_list)} | " + \
                f"{datetime.datetime.now()}"
            f.write(logstr+"\n")
            f.flush()
            print(logstr)
            break
    
    data_chunksize = len(data_list)
    # improve the efficiency of Queue's get() 
    data, slices, _ = collate(
        data_list[0].__class__,
        data_list=data_list,
        increment=False,
        add_batch=False,
    )

    del data_list

    logstr = \
        f"Process {os.getpid()} is saving processed data_list chunk " + \
        f"with size {data_chunksize} to {stash_path} ..."
    f.write(logstr+"\n")
    f.flush()
    print(logstr)

    torch.save((data, slices), stash_path)
    del data, slices
    f.close()
    return data_chunksize
    # return (data, slices)

def mp_seal_sampling(
        g, links, labels, targets, 
        num_sampler, num_hops, ratio_per_hop, 
        # we add file path to save data_list to disk
        processed_path
    ):
    r""" Multiprocessing wrapper for seal_sampling() using Pool.
    Args:
        g (Data): augmented graph,
        links (Tensor): links to be sampled,
        labels (Tensor): labels for links, used in classification,
        targets (Tensor): target cap values for links, used in regression,
        num_sampler (int): number of process,
        num_hops (int): hops used by seal_sampling,
        ratio_per_hop (float): sample ratio when do seal_sampling at each hop,
        processed_path (str): save to disk path.
    Returns:
        data_list
    """
    start = time.perf_counter() 
    pool = mp.get_context('spawn').Pool(num_sampler)
    result_list = []
    indices = torch.tensor([
        i for i in range(0, labels.size(0)-num_sampler, num_sampler)])
    # with mp.get_context('spawn').Pool(self.num_sampler) as pool:
    for i in range(num_sampler):
        # we do sampling for links or for nodes
        if links.ndim == 2 and links.size(0) == 2:
            links_for_worker = links[:, i+indices]
        else:
            links_for_worker = links[i+indices]
        labels_for_worker  = labels[i+indices]
        targets_for_worker = targets[i+indices]
        path_for_worker    = processed_path+f".stash{i}"
        args = (
            g.edge_index, g.edge_type, 
            g.node_type, g.is_directed(),
            links_for_worker, labels_for_worker, 
            targets_for_worker, # edge Cc for regression
            num_hops, ratio_per_hop,
            g.node_attr, # node feat for 
            path_for_worker,
        )
        result_list.append(pool.apply_async(func=seal_sampling, args=args))

        # data_list = list(tqdm(
        # # data_list = \
        #     pool.imap(self.seal_sampling, args_list), 
        #     total=labels.size(0), ncols=80))
    pool.close()
    pool.join()
    """
    data_slices_list = []
    for i in range(len(result_list)):
        collated_data, collated_slices = result_list[i].get()
        # NOTE (HARD BUG IS FIXED): deepcopy and del are 
        # very important for avoiding file descriptor leak!
        # Solution from https://github.com/pytorch/pytorch/issues/65198
        data_slices_list.append((
            copy.deepcopy(collated_data), 
            copy.deepcopy(collated_slices),
        ))
        del collated_data
        del collated_slices

    elapsed = time.perf_counter() - start
    timestr = time.strftime('%H:%M:%S', time.gmtime(elapsed))
    logging.info(f"{num_sampler} workers have done sampling. Took {timestr}")
    
    data_list = []
    for collated_data, collated_slices in data_slices_list:
        data_list += data_separate(collated_data, collated_slices)

    del data_slices_list
    """
    # data_list = []
    data_length = 0
    for i in range(len(result_list)):
        data_length += result_list[i].get()
        # collated_data, collated_slices = torch.load(processed_path+f".stash{i}")
        # data_list += data_separate(collated_data, collated_slices)
    del result_list
    elapsed = time.perf_counter() - start
    timestr = time.strftime('%H:%M:%S', time.gmtime(elapsed))
    logging.info(f"{num_sampler} workers have done sampling. Took {timestr}")
    return data_length

def collated_data_separate(data: Data, slices, idx: int=None):
    r""" 
    Reverse to InMemoryDataset.collate(),
    return the separated data_list from whole data chunk.
    """
    if idx is not None:
        separated_data = separate(
            cls=data.__class__,
            batch=data,
            idx=idx,
            slice_dict=slices,
            decrement=False,
        )
        return copy.copy(separated_data)
    
    data_list = []
    for i in range(data.y.size(0)):
        separated_data = separate(
            cls=data.__class__,
            batch=data,
            idx=i,
            slice_dict=slices,
            decrement=False,
        )
        data_list.append(copy.copy(separated_data))
    return data_list


class ArgsIterator:
    def __init__(self, graph, node_pairs, labels, targets):
        self.graph = graph
        self.labels = labels
        self.node_pairs = node_pairs
        self.targets = targets
        self.idx = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.idx < self.labels.size(0):
            args = tuple((
                self.graph, 
                self.node_pairs[:, self.idx].view(2,1),
                self.labels[self.idx].view(1,1), 
                self.targets[self.idx].view(1,1),
            ))
            self.idx += 1
            return args
        else:
            raise StopIteration
